{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a summary of the code explained in a series or [articles](https://medium.com/@dbenzaquenm) that aim to introduce recurrent neural networks. More precisely to the article titled *How to build a translation pipeline with RNN and Keras*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to translation file\n",
    "path_to_data = 'data/spa.txt'\n",
    "\n",
    "# Read file\n",
    "translation_file = open(path_to_data,\"r\", encoding='utf-8') \n",
    "raw_data = translation_file.read()\n",
    "translation_file.close()\n",
    "\n",
    "# Parse data\n",
    "raw_data = raw_data.split('\\n')\n",
    "pairs = [sentence.split('\\t') for sentence in  raw_data]\n",
    "pairs = pairs[:-1] # skip last empty element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs[1000:20000]\n",
    "\n",
    "for idx_sample in range(5,10):\n",
    "    print('English example in pair {}:  {}'.format(idx_sample + 1, pairs[idx_sample][0]))\n",
    "    print('Spanish example in pair {}:  {}'.format(idx_sample + 1, pairs[idx_sample][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # Lower case the sentence\n",
    "    lower_case_sent = sentence.lower()\n",
    "    # Strip punctuation\n",
    "    string_punctuation = string.punctuation + \"¡\" + '¿'\n",
    "    clean_sentence = lower_case_sent.translate(str.maketrans('', '', string_punctuation))\n",
    "   \n",
    "    return clean_sentence\n",
    "\n",
    "print(clean_sentence(\"I will surf today !!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_examples = [\n",
    "    'i will surf today',\n",
    "    'this week i will travel to the beach',\n",
    "    'he went to his house by the beach',]\n",
    "\n",
    "# Create tokenizer\n",
    "exp_text_tokenizer = Tokenizer()\n",
    "# Create word index\n",
    "exp_text_tokenizer.fit_on_texts(text_examples)\n",
    "for key, value in exp_text_tokenizer.word_index.items():\n",
    "    print(\"Word: {} is converted to number {}\".format(key, value))\n",
    "    \n",
    "    \n",
    "# Tokenize sentences\n",
    "exp_text_tokenized = exp_text_tokenizer.texts_to_sequences(text_examples)\n",
    "print ('\\n')\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_examples, exp_text_tokenized)):\n",
    "    print('Input sentence:  {}'.format(sent))\n",
    "    print('Output vector: {} \\n'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    # Create tokenizer\n",
    "    text_tokenizer = Tokenizer()\n",
    "    # Fit texts\n",
    "    text_tokenizer.fit_on_texts(sentences)\n",
    "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean sentences\n",
    "english_sentences = [clean_sentence(pair[0]) for pair in pairs]\n",
    "spanish_sentences = [clean_sentence(pair[1]) for pair in pairs]\n",
    "\n",
    "# Tokenize words\n",
    "spa_text_tokenized, spa_text_tokenizer = tokenize(spanish_sentences)\n",
    "eng_text_tokenized, eng_text_tokenizer = tokenize(english_sentences)\n",
    "\n",
    "print('Maximum length spanish sentence: {}'.format(len(max(spa_text_tokenized,key=len))))\n",
    "print('Maximum length english sentence: {}'.format(len(max(eng_text_tokenized,key=len))))\n",
    "\n",
    "# Check language length\n",
    "spanish_vocab = len(spa_text_tokenizer.word_index) + 1\n",
    "english_vocab = len(eng_text_tokenizer.word_index) + 1\n",
    "print(\"Spanish vocabulary is of {} unique words\".format(spanish_vocab))\n",
    "print(\"English vocabulary is of {} unique words\".format(english_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_examples = ['i will surf today',\n",
    "    'this week i will travel to the beach',\n",
    "    'he went to his house by the beach',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum length of example sentence: {}'.format(len(max(exp_text_tokenized,key=len))))\n",
    "# Pad tokenize vectors\n",
    "exp_pad_sentence = pad_sequences(exp_text_tokenized, 8, padding = \"post\") # 8 is the max length\n",
    "for index, pad_sentence in enumerate(exp_pad_sentence):\n",
    "    print(\"Example sentence {}:\".format(index+1))\n",
    "    print(\"  -Input:{}\".format(exp_text_tokenized[index]))\n",
    "    print(\"  -Output:{}\".format(pad_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 12\n",
    "spa_pad_sentence = pad_sequences(spa_text_tokenized, max_sentence_length, padding = \"post\")\n",
    "eng_pad_sentence = pad_sequences(eng_text_tokenized, max_sentence_length, padding = \"post\")\n",
    "\n",
    "# Reshape data\n",
    "spa_pad_sentence = spa_pad_sentence.reshape(*spa_pad_sentence.shape, 1)\n",
    "eng_pad_sentence = eng_pad_sentence.reshape(*eng_pad_sentence.shape, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simple RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Warning:</b> Tha paremeters of the model are not optimized</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_sentence(logits, tokenizer):\n",
    "\n",
    "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<empty>' \n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (max_sentence_length, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape = (max_sentence_length, 1)\n",
    "input_sequence = Input(input_shape, name='InputLayer')\n",
    "rnn = LSTM(256, return_sequences=True, dropout=0.5, name='RNNLayer')(input_sequence)\n",
    "logits = TimeDistributed(Dense(spanish_vocab), name='TimeDistributed')(rnn)\n",
    "\n",
    "model = Model(input_sequence, Activation('softmax')(logits))\n",
    "model.compile(loss=sparse_categorical_crossentropy,\n",
    "              optimizer=Adam(1e-2),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_results = model.fit(eng_pad_sentence, spa_pad_sentence, batch_size=30, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "print(\"The english sentence is: {}\".format(english_sentences[index]))\n",
    "print(\"The spanish sentence is: {}\".format(spanish_sentences[index]))\n",
    "print('The predicted sentence is :')\n",
    "print(logits_to_sentence(model.predict(eng_pad_sentence[index:index+1])[0], spa_text_tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have enjoyed this first tutorial you can find improved models in the following link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find theoretical explanation in my [Medium profile](https://medium.com/@dbenzaquenm) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:udacity] *",
   "language": "python",
   "name": "conda-env-udacity-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
